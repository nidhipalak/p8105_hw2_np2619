p8105\_hw2\_np2619
================
Nidhi Patel
9/28/2020

``` r
library(tidyverse)
```

    ## ── Attaching packages ─────────────────────────────── tidyverse 1.3.0 ──

    ## ✓ ggplot2 3.3.2     ✓ purrr   0.3.4
    ## ✓ tibble  3.0.3     ✓ dplyr   1.0.2
    ## ✓ tidyr   1.1.2     ✓ stringr 1.4.0
    ## ✓ readr   1.3.1     ✓ forcats 0.5.0

    ## ── Conflicts ────────────────────────────────── tidyverse_conflicts() ──
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(readxl)
```

## Problem 1

###### Clean up Mr. Trash Wheel

``` r
trash = 
  read_excel(
    "./Trash-Wheel-Collection-Totals-8-6-19.xlsx", 
    sheet = "Mr. Trash Wheel",
    range = "A2:N406") %>%
janitor::clean_names() %>%
drop_na(dumpster) %>%
mutate(
  sports_balls = round(sports_balls),
  sports_balls = as.integer(sports_balls)
  )
```

###### Clean up precipitation 2017 and 2018

Import datasets + clean a bit

``` r
precip17 = 
  read_excel(
    "./Trash-Wheel-Collection-Totals-8-6-19.xlsx", 
    sheet = "2017 Precipitation",
        range = "A2:B14") %>%
janitor::clean_names() %>% 
mutate(year = "2017") %>% 
  mutate(month = month.name)

precip18 = 
  read_excel(
    "./Trash-Wheel-Collection-Totals-8-6-19.xlsx", 
    sheet = "2018 Precipitation",
    range = "A2:B14") %>%
janitor::clean_names() %>% 
mutate(year = "2018") %>% 
  mutate(month = month.name)
```

###### Join datasets

``` r
precipitation = 
  bind_rows(precip17, precip18)
```

The Mr. Trash Wheel data has 344 observations after cleaned. The median
number of sports balls in a dumpster in 2017 is 8.

The precipitation data set looks at amount of rainfall in inches per
month for 2017 and 2018. In 2017, the average precipitation is 2.7441667
with the min of 0 and max of 7.09. In 2018, the average monthly
precipitation is 5.8608333 with a min of 0.94 and max of 10.47. For the
two years the average precipitation is 4.3025 with a total of 24
observations. The total precipitation in 2018 is 70.33.

## Problem 2

###### Tidy Transit data

``` r
transit = 
  read_csv("./NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names() %>% 
  select(line:entry, vending, ada) %>% 
  mutate(entry = as.logical(recode(entry, "YES" = "TRUE", "NO" = "FALSE"))) %>% 
  mutate_at(vars(route8:route11), as.character) %>% 
  pivot_longer(
    route1:route11,
    names_to = "route",
    names_prefix = "route",
    values_to = "rt_name"
  ) %>% 
  relocate(route, rt_name) %>% 
  drop_na(rt_name)
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_character(),
    ##   `Station Latitude` = col_double(),
    ##   `Station Longitude` = col_double(),
    ##   Route8 = col_double(),
    ##   Route9 = col_double(),
    ##   Route10 = col_double(),
    ##   Route11 = col_double(),
    ##   ADA = col_logical(),
    ##   `Free Crossover` = col_logical(),
    ##   `Entrance Latitude` = col_double(),
    ##   `Entrance Longitude` = col_double()
    ## )

    ## See spec(...) for full column specifications.

First, to tidy this data, I cleaned names and converted the character
‘entry’ vector to logical. This dataset had many routes that can be
combined into one column, but first all the routes have to match and
route8-route11 were converted to characters to match route1-route7. I
used `pivot_longer` to tidy the data for easier analysis. In doing so,
multiple route columns were combined to one, with their values as
‘rt\_name’. After this step, I dropped the NA values of rt\_names to
remove excess rows from creating a singular ‘route’ variable. In total,
there are 4,270 observations and 10 vectors (4,270 x 10). Variables in
the dataset include route, rt\_name (the subway line at the route),
line, station name, station latitude, station longitude, entrance type,
entry, vending, and ADA compliance.

###### Questions

``` r
distinct(transit, rt_name, line) %>% 
  count()
```

    ## # A tibble: 1 x 1
    ##       n
    ##   <int>
    ## 1   259

``` r
count(transit, ada == "TRUE")
```

    ## # A tibble: 2 x 2
    ##   `ada == "TRUE"`     n
    ##   <lgl>           <int>
    ## 1 FALSE            2654
    ## 2 TRUE             1616

``` r
filter(transit, vending == "NO") %>% 
  count()
```

    ## # A tibble: 1 x 1
    ##       n
    ##   <int>
    ## 1   447

``` r
filter(transit, vending == "NO", entry == "TRUE") %>% 
  count()
```

    ## # A tibble: 1 x 1
    ##       n
    ##   <int>
    ## 1   139

``` r
prop = 139/447
```

There are 259 distinct stations and 1616 stations are ADA compliant. The
proportion of stations without vending that allow entrance can be found
by the number of stations without vending that allow entrances/exits
(139) over the number of stations without vending(447). This is
31.0961969%

## Problem 3

Merging data using Year and month as keys across the 3 datasets.

###### clean + tidy pols-month

``` r
pols = 
  read_csv("./fivethirtyeight_datasets/pols-month.csv") %>% 
  janitor::clean_names() %>% 
  separate(mon, into = c("year", "month", "day")) %>% 
mutate(
  year = as.integer(year),
  month = as.integer(month)
  )
```

    ## Parsed with column specification:
    ## cols(
    ##   mon = col_date(format = ""),
    ##   prez_gop = col_double(),
    ##   gov_gop = col_double(),
    ##   sen_gop = col_double(),
    ##   rep_gop = col_double(),
    ##   prez_dem = col_double(),
    ##   gov_dem = col_double(),
    ##   sen_dem = col_double(),
    ##   rep_dem = col_double()
    ## )

``` r
month_df =
  tibble(
    month = 1:12,
    month_name = tolower(month.abb)
  )

 
polstidy = 
  left_join(pols, month_df, by = "month") %>% 
  relocate(prez_gop, prez_dem) %>% 
  pivot_longer(
    prez_gop:prez_dem, 
    names_to = "president",
    names_prefix = "prez_",
    values_to = "partyaffil") %>% 
    mutate(month = month_name) %>% 
    select(year:month, gov_gop:rep_dem, president:partyaffil) 
```

###### clean + tidy snp

``` r
snp = 
  read_csv("./fivethirtyeight_datasets/snp.csv") %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("month", "day", "year")) %>% 
 mutate(
  year = as.integer(year),
  month = as.integer(month)
  )
```

    ## Parsed with column specification:
    ## cols(
    ##   date = col_character(),
    ##   close = col_double()
    ## )

``` r
snp_tidy = 
  left_join(snp, month_df, by = "month") %>%
  relocate(year, month_name) %>% 
  mutate(month = month_name) %>% 
  select(year, month, close)
```

###### clean + tidy unemployment

``` r
unemploy = 
  read_csv("./fivethirtyeight_datasets/unemployment.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    jan:dec,
    names_to = "month",
    values_to = "percent"
  ) %>% 
  drop_na(percent)
```

    ## Parsed with column specification:
    ## cols(
    ##   Year = col_double(),
    ##   Jan = col_double(),
    ##   Feb = col_double(),
    ##   Mar = col_double(),
    ##   Apr = col_double(),
    ##   May = col_double(),
    ##   Jun = col_double(),
    ##   Jul = col_double(),
    ##   Aug = col_double(),
    ##   Sep = col_double(),
    ##   Oct = col_double(),
    ##   Nov = col_double(),
    ##   Dec = col_double()
    ## )

###### merge bby merge

``` r
merge1 = 
  left_join(polstidy, snp_tidy, by = c("year", "month"))
merge2 = 
  left_join(merge1, unemploy, by = c("year", "month"))
```

The first dataset, “pols-month” has the number of national politicians
who are democratic or republican at any given time. The resulting,
cleaned and tidied data set has 1644 observations across 10 variables.
Key variables are gov\_gop, sen\_gop, rep\_gop, gov\_dem, sen\_dem,
rep\_dem indicating the number of republican governers, republican
senators, republican representatives, democratic governers, democratic,
senators, democratic representatives in a given month and year. The
highest for each category is 34, 56, 253, 41, 71, 301, respectively.
This data was taken between 1947 and 2015.

The second dataset, “snp” contains the closing values of the S\&P stock
index on a given month and year (the number is taken on the first,
second or third day of the month. The highest closing value of S\&P was
2107.389893 and the lowest was 17.049999. These values were collected
between 1950 and 2015.

The last dataset, unemployed has the percentages of unemployment by
year. The highest rate of unemployment was 10.8 and the lowest rate was
2.5. This data was collected between 1948 and 2015
